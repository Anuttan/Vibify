name: Data Pipeline

on:
  workflow_dispatch:

env:
  PROJECT_ID: ${{ secrets.GCP_PROJECT_ID }}
  LOCATION: "us-east1"
  ENVIRONMENT_NAME: "data-pipeline"

jobs:
  deploy-and-run:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v3

      - name: Google Auth
        id: auth
        uses: google-github-actions/auth@v1
        with:
          credentials_json: ${{ secrets.GCP_KEY }}
          project_id: ${{ secrets.GCP_PROJECT_ID }}

      - name: Set up Cloud SDK
        uses: google-github-actions/setup-gcloud@v1
        with:
          project_id: ${{ secrets.GCP_PROJECT_ID }}

      - name: Configure gcloud project
        run: |
          gcloud config set project ${{ secrets.GCP_PROJECT_ID }}

      - name: Enable required APIs
        run: |
          gcloud services enable composer.googleapis.com
          gcloud services enable artifactregistry.googleapis.com

      - name: Check/Create Cloud Composer Environment
        run: |
          if [[ -n "$(gcloud composer environments list --locations=${{ env.LOCATION }} --project=vibify-demo --filter="name:${{ env.ENVIRONMENT_NAME }}" --format="get(name)")" ]]; then
            echo "Environment ${{ env.ENVIRONMENT_NAME }} already exists"
          else
            gcloud composer environments create ${{ env.ENVIRONMENT_NAME }} \
              --location=${{ env.LOCATION }} \
              --project=vibify-demo \
              --image-version=composer-3-airflow-2.10.2-build.0 \
              --service-account=751083964065-compute@developer.gserviceaccount.com \
              --environment-size=small \
              --scheduler-cpu=0.5 \
              --scheduler-memory=1 \
              --scheduler-storage=1 \
              --web-server-cpu=0.5 \
              --web-server-memory=1 \
              --web-server-storage=1 \
              --worker-cpu=0.5 \
              --worker-memory=1 \
              --worker-storage=1 \
              --min-workers=1 \
              --max-workers=3 \
              --storage-bucket us-east1-data-pipeline-c88b0d97-bucket 
          fi

      - name: Check and Install Python Dependencies
        run: |
          # Get current packages
          CURRENT_PACKAGES=$(gcloud composer environments describe ${{ env.ENVIRONMENT_NAME }} \
            --location=${{ env.LOCATION }} \
            --project=vibify-demo \
            --format="get(config.softwareConfig.pypiPackages)")
          
          # Read required packages
          REQUIRED_PACKAGES=$(cat pipelines/dags/requirements.txt)
          
          # Check if any package is missing
          MISSING=false
          while IFS= read -r package; do
            if ! echo "$CURRENT_PACKAGES" | grep -q "$(echo $package | cut -d'=' -f1)"; then
              MISSING=true
              break
            fi
          done <<< "$REQUIRED_PACKAGES"
          
          if [ "$MISSING" = true ]; then
            echo "Installing missing Python dependencies"
            gcloud composer environments update ${{ env.ENVIRONMENT_NAME }} \
              --location=${{ env.LOCATION }} \
              --project=vibify-demo \
              --update-pypi-packages-from-file pipelines/dags/requirements.txt
          else
            echo "All required packages are already installed"
          fi

      - name: Get Composer GCS bucket
        id: get-bucket
        run: |
          DAGS_BUCKET=$(gcloud composer environments describe ${{ env.ENVIRONMENT_NAME }} \
            --location=${{ env.LOCATION }} \
            --project=vibify-demo \
            --format="get(config.dagGcsPrefix)" | sed 's/\/dags//')
          echo "DAGS_BUCKET=$DAGS_BUCKET" >> $GITHUB_ENV

      - name: Clear existing DAGs
        run: |
          gsutil -m rm -rf ${{ env.DAGS_BUCKET }}/dags/* || true

      - name: Copy DAGs folder to GCS
        run: |
          gsutil -m cp -r pipelines/dags/* ${{ env.DAGS_BUCKET }}/dags/

      - name: List uploaded DAGs
        run: |
          echo "Listing contents of DAGs folder in GCS:"
          gsutil ls -r ${{ env.DAGS_BUCKET }}/dags/

      - name: Set Airflow Variables
        run: |
          # Wait for environment to be ready
          sleep 300
          
          # Set each variable in Airflow
          gcloud composer environments run ${{ env.ENVIRONMENT_NAME }} \
            --location=${{ env.LOCATION }} \
            --project=vibify-demo \
            variables set -- GCP_PROJECT_ID "${{ env.DAGS_BUCKET }}"

          gcloud composer environments run ${{ env.ENVIRONMENT_NAME }} \
            --location=${{ env.LOCATION }} \
            --project=vibify-demo \
            variables set -- SLACK_WEBHOOK_URL "${{ secrets.SLACK_WEBHOOK_URL }}"

      - name: Setup Git in Composer Environment
        run: |      
          gcloud composer environments run ${{ env.ENVIRONMENT_NAME }} \
            --location=${{ env.LOCATION }} \
            --project=vibify-demo \
            cli -- \
            bash -c "cd /home/airflow && git init && git config --global --add safe.directory /home/airflow"

      - name: Trigger DAG
        run: |
          gcloud composer environments run ${{ env.ENVIRONMENT_NAME }} \
            --location=${{ env.LOCATION }} \
            --project=vibify-demo \
            dags trigger -- data_pipeline

      - name: Delete Cloud Composer Environment
        run: |
          sleep 300 # To see that the dag works properly and to show in video
          gcloud composer environments delete ${{ env.ENVIRONMENT_NAME }} \
            --location=${{ env.LOCATION }} \
            --project=vibify-demo \
            --quiet