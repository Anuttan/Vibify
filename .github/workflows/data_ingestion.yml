name: Data Updation

on:
  push:
    branches:
      - main
  schedule:
    # Runs at 00:00 on the 1st of every month
    - cron: '0 0 1 * *'
  workflow_dispatch:  # Allows manual trigger

env:
  GCS_BUCKET: ${{ secrets.GCS_BUCKET }}

jobs:
  update-dataset:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        dataset:
          - {
              name: 'facial-expression',
              kaggle_id: 'nicolejyt/facialexpressionrecognition',
              gcs_path: 'data/raw/facial_expression',
              specific_file: ''
            }
          - {
              name: 'spotify-genres',
              kaggle_id: 'mrmorj/dataset-of-songs-in-spotify',
              gcs_path: 'data/raw/spotify',
              specific_file: 'genres_v2.csv'
            }
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v3

    - name: Set up Python
      uses: actions/setup-python@v4 # Sets up Python in the environment
      with:
        python-version: "==3.11"

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install kaggle google-cloud-storage

    - name: Configure Kaggle credentials
      run: |
        mkdir -p ~/.kaggle
        echo "{\"username\":\"${{ secrets.KAGGLE_USERNAME }}\",\"key\":\"${{ secrets.KAGGLE_KEY }}\"}" > ~/.kaggle/kaggle.json
        chmod 600 ~/.kaggle/kaggle.json

    - name: Configure GCP credentials
      uses: google-github-actions/auth@v1
      with:
        credentials_json: ${{ secrets.GCP_KEY }}

    - name: Download Dataset
      run: |
        kaggle datasets download -d ${{ matrix.dataset.kaggle_id }} --unzip -p ./data
        echo "Dataset contents:"
        ls -la ./data

    - name: Upload to GCS with replacement
      run: |
        python - <<EOF
        from google.cloud import storage
        import os
        import logging
        import shutil

        # Configure logging
        logging.basicConfig(level=logging.INFO)
        logger = logging.getLogger(__name__)

        def upload_to_gcs(bucket_name, source_dir, destination_path, specific_file=''):
            try:
                storage_client = storage.Client()
                bucket = storage_client.bucket(bucket_name)
                
                # Delete existing files
                logger.info(f"Deleting existing files in gs://{bucket_name}/{destination_path}")
                blobs = bucket.list_blobs(prefix=destination_path)
                for blob in blobs:
                    blob.delete()
                    logger.info(f"Deleted: gs://{bucket_name}/{blob.name}")
                
                # Upload new files
                total_files = 0
                total_size = 0
                
                # If specific file is specified, only upload that file
                if specific_file:
                    files_to_process = [os.path.join(source_dir, specific_file)]
                else:
                    files_to_process = []
                    for root, _, files in os.walk(source_dir):
                        for file in files:
                            files_to_process.append(os.path.join(root, file))

                for local_path in files_to_process:
                    if not os.path.exists(local_path):
                        logger.warning(f"File not found: {local_path}")
                        continue

                    file = os.path.basename(local_path)
                    if file.startswith('.'):
                        continue
                        
                    relative_path = os.path.basename(local_path)
                    blob_path = f"{destination_path}/{relative_path}"
                    
                    blob = bucket.blob(blob_path)
                    blob.upload_from_filename(local_path)
                    
                    size = os.path.getsize(local_path) / (1024 * 1024)  # Size in MB
                    total_size += size
                    total_files += 1
                    
                    logger.info(f"Uploaded {local_path} ({size:.2f} MB) to gs://{bucket_name}/{blob_path}")
                
                logger.info(f"Upload complete. Total files: {total_files}, Total size: {total_size:.2f} MB")
                
            except Exception as e:
                logger.error(f"Error during upload: {str(e)}")
                raise

        upload_to_gcs(
            '${{ env.GCS_BUCKET }}',
            './data',
            '${{ matrix.dataset.gcs_path }}',
            '${{ matrix.dataset.specific_file }}'
        )
        EOF

    - name: Cleanup local files
      if: always()  # Run cleanup even if previous steps fail
      run: |
        rm -rf ./data
        echo "Cleaned up local files"