name: Data Updation

on:
  schedule:
    # Runs at 00:00 on the 1st of every month
    - cron: '0 0 1 * *'
  workflow_dispatch:  # Allows manual trigger

env:
  GCS_BUCKET: ${{ secrets.GCS_BUCKET }}

jobs:
  update-dataset:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        dataset:
          - {
              name: 'facial-expression',
              kaggle_id: 'nicolejyt/facialexpressionrecognition',
              gcs_path: 'data/raw/facial_expression',
              specific_file: ''
            }
          - {
              name: 'spotify-genres',
              kaggle_id: 'mrmorj/dataset-of-songs-in-spotify',
              gcs_path: 'data/raw/spotify',
              specific_file: 'genres_v2.csv'
            }
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v3

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: "==3.11"

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install kaggle google-cloud-storage tensorflow-data-validation pandas
        pip install -r requirements.txt

    - name: Configure Kaggle credentials
      run: |
        mkdir -p ~/.kaggle
        echo "{\"username\":\"${{ secrets.KAGGLE_USERNAME }}\",\"key\":\"${{ secrets.KAGGLE_KEY }}\"}" > ~/.kaggle/kaggle.json
        chmod 600 ~/.kaggle/kaggle.json

    - name: Configure GCP credentials
      uses: google-github-actions/auth@v1
      with:
        credentials_json: ${{ secrets.GCP_KEY }}

    - name: Initialize GCS folders and upload src files
      run: |
          cat > init_gcs.py << 'EOL'
          from google.cloud import storage
          import logging
          import os
  
          logging.basicConfig(level=logging.INFO)
          logger = logging.getLogger(__name__)
  
          def ensure_gcs_folder(bucket, folder_path):
              if not folder_path.endswith('/'):
                  folder_path += '/'
              placeholder = bucket.blob(folder_path + '.placeholder')
              if not placeholder.exists():
                  placeholder.upload_from_string('')
                  logger.info(f"Created folder: gs://{bucket.name}/{folder_path}")
  
          def upload_src_files(bucket, src_dir):
              for root, _, files in os.walk(src_dir):
                  for file in files:
                      if file.endswith('.py'):
                          local_path = os.path.join(root, file)
                          relative_path = os.path.relpath(local_path, src_dir)
                          gcs_path = f"src/{relative_path}"
                          blob = bucket.blob(gcs_path)
                          blob.upload_from_filename(local_path)
                          logger.info(f"Uploaded {local_path} to gs://{bucket.name}/{gcs_path}")
  
          def initialize_bucket_structure(bucket_name: str, src_dir: str = 'src'):
              try:
                  storage_client = storage.Client()
                  bucket = storage_client.bucket(bucket_name)
                  
                  required_folders = [
                      'models/',
                      'data/preprocessed/spotify/',
                      'data/preprocessed/emotions/',
                      'data/raw/spotify/',
                      'data/raw/emotions/',
                      'src/'
                  ]
                  
                  for folder in required_folders:
                      ensure_gcs_folder(bucket, folder)
                      
                  logger.info(f"Successfully initialized folder structure in bucket: {bucket_name}")
                  
                  if os.path.exists(src_dir):
                      logger.info(f"Uploading files from {src_dir} to gs://{bucket_name}/src/")
                      upload_src_files(bucket, src_dir)
                  else:
                      logger.warning(f"Source directory {src_dir} not found")
                  
              except Exception as e:
                  logger.error(f"Error initializing bucket structure: {str(e)}")
                  raise
  
          if __name__ == '__main__':
              import sys
              bucket_name = sys.argv[1]
              src_dir = sys.argv[2] if len(sys.argv) > 2 else 'src'
              initialize_bucket_structure(bucket_name, src_dir)
          EOL
          python init_gcs.py ${{ env.GCS_BUCKET }} src

    - name: Download Dataset
      run: |
        kaggle datasets download -d ${{ matrix.dataset.kaggle_id }} --unzip -p ./data
        echo "Dataset contents:"
        ls -la ./data

    - name: Run Anomaly Detection
      run: python src/anomaly_detection.py ./data '${{ matrix.dataset.specific_file }}'
  
    - name: Upload to GCS with replacement
      run: python src/gcs_upload.py '${{ env.GCS_BUCKET }}' './data' '${{ matrix.dataset.gcs_path }}' '${{ matrix.dataset.specific_file }}'

    - name: Bias Mitigation
      run: python src/bias_mitigation.py '${{ env.GCS_BUCKET }}' './data' '${{ matrix.dataset.specific_file }}'

    - name: Cleanup local files
      if: always()
      run: |
        rm -rf ./data
        echo "Cleaned up local files"